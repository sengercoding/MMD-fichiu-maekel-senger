{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_4PlFo8maSj"
      },
      "source": [
        "# MMD 2024, Problem Sheet 2: Latent Factor Models\n",
        "\n",
        "Group: Daniela Fichiu, Aaron Maekel, Manuel Senger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D0cxz7umaSj"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "**Task:** Consider a web shop that sells furniture and uses a recommendation system. When a\n",
        "new user creates an account and likes one product, he will be presented with similar\n",
        "products on his next visit.\n",
        "\n",
        "How can a competitor - in principle - try to steal the valuable data for recommendation\n",
        "from this website? Does this work better when the web shop implemented a content-\n",
        "based (CB) or a collaborative filtering (CF) system? What data would the competitor be able to\n",
        "infer? Would this technique have an impact on the recommendation system, i.e., would\n",
        "this attack create a bias on the data? Why is this attack probably not viable in any\n",
        "case?\n",
        "\n",
        "**Solution:** By liking a product and receiving a list of similar products, the competitor can infer which products are similar to the one liked. This works better when the shop implements a CB approach. On one hand, the cold-start problem specific to CF disappears. On the other hand, user-user CF is based on matching a specific user to all the others. That is, a CF approach would mean the competitor would have to create specific users to receive recommendations for this specific user. The competitor would be able to infer which products are similar to the liked product. While a slight bias might be thinkable in the CF case, irrespective of the recommendation approach, nothing would happen. In the CB setting this is due to the features being hand-crafted. For the user-user CF setting, the competitor would have to create user vectors somewhat similar to the existing ones and additionally like items that the existing user would have never considered. The attack is not viable since it would require the creation of many user accounts which would like a single item or a very small set of items. This activity, however, would most certainly be detected by the server hosting the web shop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MW7woAmaSk"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "The following table shows a utility matrix with ratings on a 1-5 star scale of eight items, a through h, by three users, A, B and C.\n",
        "\n",
        "|   | a | b | c | d | e | f | g | h |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A | 4 | 5 |   | 5 | 1 |   | 3 | 2 |\n",
        "| B |   | 3 | 4 | 3 | 1 | 2 | 1 |   |\n",
        "| C | 2 |   | 1 | 3 |   | 4 | 5 | 3 |\n",
        "\n",
        "Perform the following tasks. Submit only your results (no code, even if you have used any).\n",
        "\n",
        "a) **Task:** Treating each blank entry in the utility matrix as 0, compute the cosine distance between each pair of users.\n",
        "\n",
        "**Solution:**\n",
        "The cosine distance is defined as $1 -$ the cosine similarity. Therefore, we first compute the dot products between the vectors and their norms, and then the cosine similarites.\n",
        "\n",
        "Norms:\n",
        "- $\\Vert A \\Vert_2 = (4^2 + 5^2 + 5^2 + 1 + 3^2 + 2^2)^{1/2} \\approx 9$\n",
        "- $\\Vert B \\Vert_2 = (3^2 + 4^2 + 3^2 + 1 + 2^2 + 1)^{1/2} \\approx 6$\n",
        "- $\\Vert C \\Vert_2 = (2^2 + 1 + 3^2 + 4^2 + 5^2 + 3^2)^{1/2} = 8$\n",
        "\n",
        "Dot products:\n",
        "- $ A \\cdot B = (4 \\cdot 0 + 5 \\cdot 3 + 0 \\cdot 4 + 5 \\cdot 3 + 1 \\cdot 1 + 0 \\cdot 2 + 3 \\cdot 1 + 2 \\cdot 0) = 34$\n",
        "- $ A \\cdot C = (4 \\cdot 2 + 5 \\cdot 0 + 0 \\cdot 1 + 5 \\cdot 3 + 1 \\cdot 0 + 0 \\cdot 4 + 3 \\cdot 5 + 2 \\cdot 3) = 44$\n",
        "- $ B \\cdot C = (0 \\cdot 2 + 3 \\cdot 0 + 4 \\cdot 1 + 3 \\cdot 3 + 1 \\cdot 0 + 2 \\cdot 4 + 1 \\cdot 5 + 0 \\cdot 3) = 26$\n",
        "\n",
        "Cosine similarities:\n",
        "- $\\cos_\\text{sim}(A, B) = A \\cdot B / \\Vert A \\Vert_2 \\Vert B \\Vert_2 = 34 / 9 \\cdot 6 \\approx 0.6$\n",
        "- $\\cos_\\text{sim}(A, C) = A \\cdot C / \\Vert A \\Vert_2 \\Vert C \\Vert_2 = 44 / 9 \\cdot 8 \\approx 0.6$\n",
        "- $\\cos_\\text{sim}(B, C) = B \\cdot C / \\Vert B \\Vert_2 \\Vert C \\Vert_2 = 26 / 6 \\cdot 8 \\approx 0.55$\n",
        "\n",
        "Cosiene distances:\n",
        "- $\\cos_\\text{dist}(A, B) = 1 - \\cos_\\text{sim}(A, B) \\approx 1 - 0.6 \\approx 0.4$\n",
        "- $\\cos_\\text{dist}(A, C) = 1 - \\cos_\\text{sim}(A, C) \\approx 1 - 0.6 \\approx 0.4$\n",
        "- $\\cos_\\text{dist}(B, C) = 1 - \\cos_\\text{sim}(B, C) \\approx 1 - 0.55 \\approx 0.45$\n",
        "\n",
        "\n",
        "b) **Task:** Treating ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0, compute the cosine distance between each pair of users. Compare these results to those obtain in Part a).\n",
        "\n",
        "**Solution:** Converting 3 and 4 to 1 and 5 to 2, we obtain the following utility matrix:\n",
        "\n",
        "\n",
        "|   | a | b | c | d | e | f | g | h |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A | 1 | 2 |   | 2 | 1 |   | 1 | 2 |\n",
        "| B |   | 1 | 1 | 1 | 1 | 2 | 1 |   |\n",
        "| C | 2 |   | 1 | 1 |   | 1 | 2 | 1 |\n",
        "\n",
        "For the obtained utility matrix, we repeat the steps from Part a).\n",
        "\n",
        "Norms:\n",
        "- $\\Vert A \\Vert_2 = (1^2 + 2^2 + 2^2 + 1 + 1^2 + 2^2)^{1/2} \\approx 4$\n",
        "- $\\Vert B \\Vert_2 = (1^2 + 1^2 + 1^2 + 1 + 2^2 + 1)^{1/2} \\approx 3$\n",
        "- $\\Vert C \\Vert_2 = (2^2 + 1 + 1^2 + 1^2 + 2^2 + 1^2)^{1/2} \\approx 3.5$\n",
        "\n",
        "Dot products:\n",
        "- $ A \\cdot B = (1 \\cdot 0 + 2 \\cdot 1 + 0 \\cdot 1 + 2 \\cdot 1 + 1 \\cdot 1 + 0 \\cdot 2 + 1 \\cdot 1 + 2 \\cdot 0) = 6$\n",
        "- $ A \\cdot C = (1 \\cdot 2 + 2 \\cdot 0 + 0 \\cdot 1 + 2 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 2 + 2 \\cdot 1) = 8$\n",
        "- $ B \\cdot C = (0 \\cdot 2 + 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0 + 2 \\cdot 1 + 1 \\cdot 2 + 0 \\cdot 1) = 6$\n",
        "\n",
        "Cosine similarities:\n",
        "- $\\cos_\\text{sim}(A, B) = A \\cdot B / \\Vert A \\Vert_2 \\Vert B \\Vert_2 = 6 / 4 \\cdot 3 \\approx 0.5$\n",
        "- $\\cos_\\text{sim}(A, C) = A \\cdot C / \\Vert A \\Vert_2 \\Vert C \\Vert_2 = 8 / 4 \\cdot 3.5 \\approx 0.57$\n",
        "- $\\cos_\\text{sim}(B, C) = B \\cdot C / \\Vert B \\Vert_2 \\Vert C \\Vert_2 = 6 / 3 \\cdot 3.5 \\approx 0.57$\n",
        "\n",
        "Cosine distances:\n",
        "- $\\cos_\\text{dist}(A, B) = 1 - \\cos_\\text{sim}(A, B) \\approx 1 - 0.5 \\approx 0.5$\n",
        "- $\\cos_\\text{dist}(A, C) = 1 - \\cos_\\text{sim}(A, C) \\approx 1 - 0.57 \\approx 0.43$\n",
        "- $\\cos_\\text{dist}(B, C) = 1 - \\cos_\\text{sim}(B, C) \\approx 1 - 0.57 \\approx 0.43$\n",
        "\n",
        "The results obtained are similar to those from Part a). The biggest changes are in the cosine distances of A. This is due to the fact that from the 6 non-blank entries of A (of which two were 5), four got changed.\n",
        "\n",
        "c) **Task:** Normalize the matrix by substracting from each non-blank entry the average value for its user. Then compute the cosine distance between each pair of users (blank entries are treated as 0).\n",
        "\n",
        "**Solution:** We compute the averages only over the existing values:\n",
        "\n",
        "Averages:\n",
        "- $A = \\frac{(4 + 5 + 5 + 1 + 3 + 2)}{6} \\approx 3.3$\n",
        "- $B = \\frac{(3 + 4 + 3 + 1 + 2 + 1)}{6} \\approx 2.3$\n",
        "- $C = \\frac{(2 + 1 + 3 + 4 + 5 + 3)}{6} = 3$\n",
        "\n",
        "The normalized matrix has is then given by\n",
        "\n",
        "\n",
        "|   | a | b | c | d | e | f | g | h |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| A | 0.7 | 1.7 |   | 1.7 | -2.3 |   | -0.3 | -1.3 |\n",
        "| B |   | 0.7 | 1.7 | 0.7 | -1.3 | -0.3 | -1.3 |   |\n",
        "| C | -1 |   | -2 | 0 |   | 1 | 2 | 0 |\n",
        "\n",
        "\n",
        "Norms:\n",
        "- $\\Vert A \\Vert_2 \\approx 3.6$\n",
        "- $\\Vert B \\Vert_2 \\approx 2.7$\n",
        "- $\\Vert C \\Vert_2 \\approx 3.1$\n",
        "\n",
        "Dot products:\n",
        "- $ A \\cdot B \\approx 5.7$\n",
        "- $ A \\cdot C \\approx -1.29$\n",
        "- $ B \\cdot C \\approx -6.3$\n",
        "\n",
        "Cosine similarities:\n",
        "- $\\cos_\\text{sim}(A, B) = A \\cdot B / \\Vert A \\Vert_2 \\Vert B \\Vert_2 = 5.7 / (3.6 \\cdot 2.7) \\approx 0.5$\n",
        "- $\\cos_\\text{sim}(A, C) = A \\cdot C / \\Vert A \\Vert_2 \\Vert C \\Vert_2 = -1.29 / (3.6 \\cdot 3.1) \\approx -0.1$\n",
        "- $\\cos_\\text{sim}(B, C) = B \\cdot C / \\Vert B \\Vert_2 \\Vert C \\Vert_2 = -6.3 / (2.7 \\cdot 3.1) \\approx -0.7$\n",
        "\n",
        "Cosine distances:\n",
        "- $\\cos_\\text{dist}(A, B) = 1 - \\cos_\\text{sim}(A, B) \\approx 1 - 0.5 \\approx 0.5$\n",
        "- $\\cos_\\text{dist}(A, C) = 1 - \\cos_\\text{sim}(A, C) \\approx 1 + 0.1 \\approx 1.1$\n",
        "- $\\cos_\\text{dist}(B, C) = 1 - \\cos_\\text{sim}(B, C) \\approx 1 + 0.7 \\approx 1.7$\n",
        "\n",
        "d) **Task:** Compute the Pearson correlation coefficient between each pair of users as defined in Lecture 1 (slide \"From Cosine to Pearson\"). Compare these results to those obtained in Part c) and state your conclusions.\n",
        "\n",
        "**Solution:** The only difference between the Pearson coefficient and the cosine similarites computed in Part c) is that the Pearson coefficient computes the norm-like denominators over the commonly rated items.\n",
        "\n",
        "Denominators for the cosine similarity between:\n",
        "- $A$ and $B$: $d_{AB} \\approx 7$ (over items b, d, e, g)\n",
        "- $A$ and $C$: $d_{AC} \\approx 5$ (over items a, d, g, h)\n",
        "- $B$ and $C$: $d_{BC} \\approx 7$ (over items c, d, f, g)\n",
        "\n",
        "Cosine similarities:\n",
        "- $\\cos_\\text{sim}(A, B) = A \\cdot B / d_{AB} \\approx 5.7 / 7 \\approx 0.8$\n",
        "- $\\cos_\\text{sim}(A, C) = A \\cdot C / d_{AC} \\approx -1.29 / 5 \\approx -0.2$\n",
        "- $\\cos_\\text{sim}(B, C) = B \\cdot C / d_{BC} \\approx -6.3 / 7 \\approx -0.9$\n",
        "\n",
        "Cosine distances:\n",
        "- $\\cos_\\text{dist}(A, B) = 1 - \\cos_\\text{sim}(A, B) \\approx 1 - 0.8 \\approx 0.2$\n",
        "- $\\cos_\\text{dist}(A, C) = 1 - \\cos_\\text{sim}(A, C) \\approx 1 + 0.2 \\approx 1.2$\n",
        "- $\\cos_\\text{dist}(B, C) = 1 - \\cos_\\text{sim}(B, C) \\approx 1 + 0.9 \\approx 1.9$\n",
        "\n",
        "TODO: Write comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgaQ31omaSk"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBocwEpmaSk"
      },
      "source": [
        "Consider the function `uv_factorization_vec_no_reg` from `rec_sys/lf_algorithms.py` and the functions utilized there. Perform the following tasks:\n",
        "\n",
        "a) **Task:** Implement the function `uv_factorization_vec_reg` (along with any necessary helper or sub-functions) which executes the SDG using a loss function with the regularization terms discussed in the lecture. Use one common regularization parameter for both matrices. Test your implementation using the settings from `rec_sys/config.py` (you might need to adjust  some hyperparameters). Then, use the function `show_metrics_and_examples` to compare the convergence and the accuracy of your function against the non-regularizd version of SGD from the original (i.e., repository) code.\n",
        "\n",
        "**Solution:** See `uv_factorization_vec_reg` in `rec_sys/lf_algorithms.py`.\n",
        "\n",
        "b) **Solution:** The grid search was done in 'lf_algorithms_grid_search.py' , we tried the learning rates  [0.1 , 0.05, 0.01, 0.005, 0.001] and the regularization parameters [0.0001,0.0005, 0.001, 0.005 ,0.01  ].\n",
    "We compared them after the MSE over all test data.\n",
    "\n",
    "|reg\\lr  | 0.1 | 0.05 | 0.01 | 0.005 | 0.001 |\n",
    "|--------|-----|------|------|-------|-------| \n",
    "| 0.0001 |3.08 | 0.98 | 1.12 | 1.21  | 1.46  | \n",
    "| 0.0005 |1.65 | 0.96 | 1.00 | 1.03  | 1.34  | \n",
    "| 0.001  |1.31 | 0.95 | 0.98 | 0.99  | 1.26  | \n",
    "| 0.005  |1.46 | 1.31 | 1.22 | 1.14  | 1.26  | \n",
    "| 0.01   |2.57 | 2.43 | 1.98 | 1.64  | 1.38  | \n",
    "\n",
    "The winner was lr= 0.05 , reg = 0.001. We compared this to the original Algorithm, again the criterium was the MSE on the test data set:\n",
    "\n",
    "Original Algorithm:  1.1605393\n",
    "\n",
    "fixed_lr and with reg:    1.0945647\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "\n",
    "If the maximum Degree is N for one variable, than it can be in the polynomial part as {0,1,...,N} variants.\n",
    "THis means that there are (N_x + 1 ) * (N_Y + 1)  * ( N_z + 1) possible  combinations of these variables resulting in the same amount of coefficients.\n",
    "\n",
    "Version 1: Shape: [t,4] (x,y,z, and coeff) (dependent on t)\n",
    "\n",
    "Version 2:This would result in an three dimensional matrix having the shape [((N_x + 1 ), (N_Y + 1),( N_z + 1))] (independent of t )\n",
    "\n",
    "we tried implementing both versions, as we were unsure how to proceed, in both cases problems arised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import rec_sys.jax_intro as jax_intro\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import time\n",
    "import random\n",
    "rnd_seed = int(time.time())\n",
    "rnd_key = jax.random.PRNGKey(rnd_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example polynomial based on Polynomial configuration 2\n",
      "[[ 0  1  0  0]\n",
      " [ 3  0  2  2]\n",
      " [ 0  1  2  1]\n",
      " [ 1  0  0 -4]\n",
      " [ 2  1  2 -5]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def gen_polynomial(Nx,Ny,Nz,t):\n",
    "    \n",
    "    max_arr = [Nx,Ny,Nz]\n",
    "    \n",
    "    \n",
    "    indx = jax.random.randint(  rnd_key, (t,3), jnp.zeros((t,3)),\n",
    "                                jnp.reshape(jnp.repeat(jnp.array([Nx+1,Ny+1,Nz+1]),repeats=t,axis=0 ),\n",
    "                                newshape=(3,t)).T)\n",
    "    \n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        if max(indx[:,i])< max_arr[i] :\n",
    "            rnd_indx = random.randint(0, t-1)\n",
    "            indx = indx.at[rnd_indx,i].set(max_arr[i]) \n",
    "    LIM = 5\n",
    "    rnd_coeff = jax.random.randint(  rnd_key, (t,1), -LIM * jnp.ones((t,1)),\n",
    "                                    LIM * jnp.ones((t,1))\n",
    "                                     )\n",
    "    coeff_mat = jnp.hstack([indx,rnd_coeff])\n",
    "    return(coeff_mat)\n",
    "\n",
    "\n",
    "\n",
    "def f(x,y,z,polym):\n",
    "    res = jnp.zeros(x.shape[0])\n",
    "    for i in range(polym.shape[0]):\n",
    "        res += polym[i,3] * jnp.power(x,polym[i,0])* jnp.power(y,polym[i,1])* jnp.power(z,polym[i,2])\n",
    "    return(res)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_noisy_set(N,coeff_mat):\n",
    "    noise_frac, rnd_seed =  0.25, int(time.time())\n",
    "    x,y,z = jnp.linspace(-3, 3,N),jnp.linspace(-3, 3,N),jnp.linspace(-3, 3,N)\n",
    "    res_pure = f(x,y,z,coeff_mat)\n",
    "    # Add some noise to data\n",
    "    rnd_key = jax.random.PRNGKey(rnd_seed)\n",
    "    y_with_noise = res_pure + res_pure * noise_frac * jax.random.normal(rnd_key, (N,))\n",
    "    return x,y,z,y_with_noise\n",
    "\n",
    "polym = gen_polynomial(3,1,2,5)\n",
    "print(\"Example polynomial based on Polynomial configuration 2\")\n",
    "print(polym)\n",
    "#print(generate_noisy_set(10,polym) )\n",
    "\n",
    "#print(generate_noisy_set(2,jnp.array([[1,1,1,1],[0,0,0,0]])) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example polynomial based on Polynomial configuration 2\n",
      "[[[ 0.  0.  0.]\n",
      "  [ 0.  0.  3.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [-1.  0.  0.]]\n",
      "\n",
      " [[ 0. -1.  0.]\n",
      "  [ 0. -1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  1.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "def gen_polynomial2(Nx,Ny,Nz,t):\n",
    "    LIM = 3\n",
    "    rnd_seed = int(time.time())\n",
    "    max_arr = [Nx,Ny,Nz]\n",
    "\n",
    "    coeff_mat = jnp.zeros((Nx+1,Ny+1,Nz+1))\n",
    "    for _ in range(t):\n",
    "        rnd_coeff = random.randint(-LIM,LIM)\n",
    "        while rnd_coeff ==0:\n",
    "            rnd_coeff = random.randint(-LIM,LIM)\n",
    "        coeff_mat = coeff_mat.at[random.randint(0,Nx),random.randint(0,Ny),random.randint(0,Nz)].set(rnd_coeff) \n",
    "\n",
    "\n",
    "    while not jnp.any(coeff_mat[-1,:,:]):\n",
    "        coeff_mat = jnp.roll(coeff_mat,1,0)\n",
    "    while not jnp.any(coeff_mat[:,-1,:]):\n",
    "        coeff_mat = jnp.roll(coeff_mat,1,1)\n",
    "    while not jnp.any(coeff_mat[...,-1]):\n",
    "        coeff_mat = jnp.roll(coeff_mat,1,2)\n",
    "    return coeff_mat\n",
    "\n",
    "\n",
    "def f2(x,y,z,polym):\n",
    "    res = jnp.zeros(x.shape[0])\n",
    "    for i in range(polym.shape[0]):\n",
    "        for j in range(polym.shape[1]):\n",
    "            for k in range(polym.shape[2]):\n",
    "                res += polym[i,j,k] * jnp.power(x,i)* jnp.power(y,j)* jnp.power(z,k)\n",
    "    return(res)\n",
    "\n",
    "def generate_noisy_set2(N,coeff_mat):\n",
    "    noise_frac, rnd_seed =  0.25, int(time.time())\n",
    "    x,y,z = jnp.linspace(-3, 3,N),jnp.linspace(-3, 3,N),jnp.linspace(-3, 3,N)\n",
    "    res_pure = f2(x,y,z,coeff_mat)\n",
    "    # Add some noise to data\n",
    "\n",
    "    res_with_noise = res_pure + res_pure * noise_frac * jax.random.normal(rnd_key, (N,))\n",
    "    return x,y,z,res_with_noise\n",
    "\n",
    "polym = gen_polynomial2(3,1,2,5)\n",
    "print(\"Example polynomial based on Polynomial configuration 2\")\n",
    "print(polym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Running Stochastic Gradient Descent =====\n",
      "Epoch 0: param_w=[[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]], grad=[[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]], loss=nan\n",
      "[[ 0  1  0  0]\n",
      " [ 3  0  2  2]\n",
      " [ 0  1  2  1]\n",
      " [ 1  0  0 -4]\n",
      " [ 2  1  2 -5]]\n",
      "[[nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]\n",
      " [nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#polynomials:\n",
    "polym1 = gen_polynomial(2,4,6,12)\n",
    "polym2 = gen_polynomial(3,1,2,5)\n",
    "\n",
    "def SGD(polynomial,Nx,Ny,Nz,t):\n",
    "\n",
    "    # %% Run the SG loop\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.05\n",
    "    train_set_size = 10000\n",
    "    train_ds  = jnp.asarray(generate_noisy_set(train_set_size ,polynomial))\n",
    "    \n",
    "    param_w = gen_polynomial(Nx,Ny,Nz,t).astype(float)  # Initial guess for the parameter\n",
    "\n",
    "    # %% Define a simple loss function and its gradient\n",
    "    def loss(param_w, data):\n",
    "        # return  jnp.sum((data[:,1] - f(data[:,0], param_w))**2)\n",
    "        res = f(data[0],data[1],data[2], param_w)\n",
    "\n",
    "        return jnp.log(jnp.sum((data[3] - res) ** 2)) \n",
    "\n",
    "\n",
    "    # Using JAX automatic differentiation - autograd\n",
    "    grad_loss = jax.grad(loss)#,allow_int=True)\n",
    "\n",
    "    num_points_per_batch = train_set_size // 5\n",
    "    print(\"\\n===== Running Stochastic Gradient Descent =====\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get points for the current batch\n",
    "        for i in range(0, train_set_size, num_points_per_batch):\n",
    "            batch = train_ds[:,i:i + num_points_per_batch]\n",
    "            grad = grad_loss(param_w, batch)\n",
    "            param_w = param_w - learning_rate * grad\n",
    "\n",
    "        print(f\"Epoch {epoch}: param_w={param_w}, grad={grad}, loss={loss(param_w, train_ds)}\")\n",
    "\n",
    "    print(polynomial)\n",
    "    print(param_w)\n",
    "SGD(polym2,3,1 ,2,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of the gradient correlating for the x y and z data became NaN, thus destroying the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.]\n",
      "  [ 0. -2.  0.]]\n",
      "\n",
      " [[ 0.  0. -2.]\n",
      "  [ 0. -1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0. -3.]]]\n",
      "\n",
      "===== Running Stochastic Gradient Descent =====\n",
      "Epoch 0:  loss=22.013757705688477\n",
      "Epoch 1:  loss=21.905046463012695\n",
      "Epoch 2:  loss=21.78440284729004\n",
      "Epoch 3:  loss=21.647014617919922\n",
      "Epoch 4:  loss=21.48792839050293\n",
      "Epoch 5:  loss=21.303478240966797\n",
      "Epoch 6:  loss=21.08243179321289\n",
      "Epoch 7:  loss=20.8350887298584\n",
      "Epoch 8:  loss=20.543737411499023\n",
      "Epoch 9:  loss=20.33692741394043\n",
      "Epoch 10:  loss=20.09905433654785\n",
      "Epoch 11:  loss=19.927234649658203\n",
      "Epoch 12:  loss=19.749502182006836\n",
      "Epoch 13:  loss=19.660423278808594\n",
      "Epoch 14:  loss=19.583248138427734\n",
      "Epoch 15:  loss=19.563278198242188\n",
      "Epoch 16:  loss=19.52655029296875\n",
      "Epoch 17:  loss=19.523616790771484\n",
      "Epoch 18:  loss=19.48839569091797\n",
      "Epoch 19:  loss=19.493303298950195\n",
      "Epoch 20:  loss=19.469438552856445\n",
      "Epoch 21:  loss=19.472843170166016\n",
      "Epoch 22:  loss=19.438831329345703\n",
      "Epoch 23:  loss=19.43595314025879\n",
      "Epoch 24:  loss=19.442792892456055\n",
      "Epoch 25:  loss=19.416461944580078\n",
      "Epoch 26:  loss=19.416065216064453\n",
      "Epoch 27:  loss=19.42388343811035\n",
      "Epoch 28:  loss=19.400354385375977\n",
      "Epoch 29:  loss=19.40540885925293\n",
      "Epoch 30:  loss=19.39287567138672\n",
      "Epoch 31:  loss=19.404157638549805\n",
      "Epoch 32:  loss=19.38260269165039\n",
      "Epoch 33:  loss=19.38979721069336\n",
      "Epoch 34:  loss=19.374271392822266\n",
      "Epoch 35:  loss=19.377588272094727\n",
      "Epoch 36:  loss=19.37980079650879\n",
      "Epoch 37:  loss=19.36992645263672\n",
      "Epoch 38:  loss=19.382402420043945\n",
      "Epoch 39:  loss=19.373706817626953\n",
      "Epoch 40:  loss=19.36064338684082\n",
      "Epoch 41:  loss=19.370582580566406\n",
      "Epoch 42:  loss=19.354694366455078\n",
      "Epoch 43:  loss=19.362871170043945\n",
      "Epoch 44:  loss=19.3491268157959\n",
      "Epoch 45:  loss=19.349275588989258\n",
      "Epoch 46:  loss=19.36125373840332\n",
      "Epoch 47:  loss=19.35488510131836\n",
      "Epoch 48:  loss=19.345439910888672\n",
      "Epoch 49:  loss=19.35390281677246\n",
      "[[[ 0.  0.  0.]\n",
      "  [ 0. -2.  0.]]\n",
      "\n",
      " [[ 0.  0. -2.]\n",
      "  [ 0. -1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0. -3.]]]\n",
      "[[[ 0.24658726 -0.03951892 -0.2870798 ]\n",
      "  [-0.03951892 -0.28707972 -0.40288696]]\n",
      "\n",
      " [[-0.03951892 -0.28707972 -0.40288696]\n",
      "  [-0.28707972 -0.4028869  -0.19080453]]\n",
      "\n",
      " [[-0.2870798  -0.40288696 -0.19080448]\n",
      "  [-0.40288696 -0.19080451 -0.09062378]]\n",
      "\n",
      " [[-0.40288696 -0.19080445 -0.09062372]\n",
      "  [-0.19080445 -0.09062375 -2.7721086 ]]]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#polynomials:\n",
    "polym1 = gen_polynomial2(2,4,6,12)\n",
    "polym2 = gen_polynomial2(3,1,2,5)\n",
    "\n",
    "print(polym2)\n",
    "def SGD(polynomial,Nx,Ny,Nz,t):\n",
    "\n",
    "    # %% Run the SG loop\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.05\n",
    "    train_set_size = 10000\n",
    "    train_ds  = jnp.asarray(generate_noisy_set2(train_set_size ,polynomial))\n",
    "    \n",
    "    param_w = jnp.zeros((Nx+1,Ny+1,Nz+1)) #gen_polynomial2(Nx,Ny,Nz,t).astype(float)  # Initial guess for the parameter\n",
    "\n",
    "    # %% Define a simple loss function and its gradient\n",
    "    def loss(param_w, data):\n",
    "        # return  jnp.sum((data[:,1] - f(data[:,0], param_w))**2)\n",
    "        res = f2(data[0],data[1],data[2], param_w)\n",
    "      \n",
    "        return jnp.log(jnp.sum((data[3] - res) ** 2)) \n",
    "\n",
    "    # Using JAX automatic differentiation - autograd\n",
    "    grad_loss = jax.grad(loss)\n",
    "\n",
    "    num_points_per_batch = train_set_size // 5\n",
    "    print(\"\\n===== Running Stochastic Gradient Descent =====\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get points for the current batch\n",
    "        for i in range(0, train_set_size, num_points_per_batch):\n",
    "            batch = train_ds[:,i:i + num_points_per_batch]\n",
    "            grad = grad_loss(param_w, batch)\n",
    "            param_w = param_w - learning_rate * grad\n",
    "\n",
    "        print(f\"Epoch {epoch}:  loss={loss(param_w, train_ds)}\")\n",
    "    print(polynomial)\n",
    "    print(param_w)\n",
    "SGD(polym2,3,1,2,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it manages to get approximate some cells, but because every variable combination is possible, does this show a very slow convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different options for tracking memory allocation in JAX. Firstly there is the built-in JAX.profiler, which includes the method device-memory-profile. This captures a JAX device memory profile as pprof-format protocol buffer, which you can read with the pprof-tool. As far as i understand this that only works with go. I also looked at jax-smi, which is supposed to be a \"a tool for real-time inspection of the memory usage of a JAX process\". This also returns a pprof file. Reading this file takes me some time and im trying different solutions on windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different options for tracking memory allocation in JAX. Firstly there is the built-in JAX.profiler, which includes the method device-memory-profile. This captures a JAX device memory profile as pprof-format protocol buffer, which you can read with the pprof-tool. As far as i understand this that only works with go. I also looked at jax-smi, which is supposed to be a \"a tool for real-time inspection of the memory usage of a JAX process\". This also returns a pprof file. Reading this file takes me some time and im trying different solutions on windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax-smi in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: jax>=0.2.16 in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax-smi) (0.4.23)\n",
      "Requirement already satisfied: fire in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax-smi) (0.7.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax>=0.2.16->jax-smi) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax>=0.2.16->jax-smi) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax>=0.2.16->jax-smi) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from jax>=0.2.16->jax-smi) (1.13.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\aaron maekel\\.conda\\envs\\mmd\\lib\\site-packages (from fire->jax-smi) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jax-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "from jax_smi import initialise_tracking\n",
    "import pandas as pd\n",
    "import pstats\n",
    "import jax.profiler\n",
    "from datetime import datetime\n",
    "import tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test matrix u and v to test memory allocation using jax-smi\n",
    "key = jax.random.PRNGKey(0)\n",
    "size = 1000\n",
    "num_choices = 50\n",
    "mat_u = jax.random.normal(jax.random.PRNGKey(0), (1000, 1000))  # as initialized in init_latent_factors\n",
    "mat_v = jax.random.normal(jax.random.PRNGKey(1), (1000, 1000))\n",
    "rows = jax.random.choice(key, jnp.arange(size), (num_choices,), replace=False)\n",
    "columns = jax.random.choice(key, jnp.arange(size), (num_choices,), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jax_smi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(mat_u \u001b[38;5;241m@\u001b[39m mat_v)[(rows, columns)]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmse_loss_one_batch_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mmse_loss_one_batch_test\u001b[1;34m(mat_u, mat_v, rows, columns)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit  \u001b[38;5;66;03m# Comment out for single-step debugging\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmse_loss_one_batch_test\u001b[39m(mat_u, mat_v, rows, columns):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Original code in question\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mjax_smi\u001b[49m\u001b[38;5;241m.\u001b[39minitialise_tracking()\n\u001b[0;32m      5\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(mat_u \u001b[38;5;241m@\u001b[39m mat_v)[(rows, columns)]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jax_smi' is not defined"
     ]
    }
   ],
   "source": [
    "@jax.jit  # Comment out for single-step debugging\n",
    "def mse_loss_one_batch_test(mat_u, mat_v, rows, columns):\n",
    "    # Original code in question\n",
    "    jax_smi.initialise_tracking()\n",
    "    estimator = -(mat_u @ mat_v)[(rows, columns)]\n",
    "\n",
    "    return estimator\n",
    "\n",
    "mse_loss_one_batch_test(mat_u, mat_v, rows, columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5"
      ],
      "metadata": {
        "id": "AddszXzummpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different options for tracking memory allocation in JAX. Firstly there is the built-in JAX.profiler, which includes the method device-memory-profile. This captures a JAX device memory profile as pprof-format protocol buffer, which you can read with the pprof-tool. As far as i understand this that only works with go. I also looked at jax-smi, which is supposed to be a \"a tool for real-time inspection of the memory usage of a JAX process\". This also returns a pprof file. Reading this file takes me some time and im trying different solutions on windows."
      ],
      "metadata": {
        "id": "21Vsr1zn4CF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jax-smi"
      ],
      "metadata": {
        "id": "LE8Hy2P8mq1l",
        "outputId": "84f78e94-2918-4396-e3d1-75094e12e339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax-smi in /usr/local/lib/python3.10/dist-packages (1.0.4)\n",
            "Requirement already satisfied: jax>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from jax-smi) (0.4.33)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from jax-smi) (0.7.0)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.2.16->jax-smi) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.2.16->jax-smi) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax>=0.2.16->jax-smi) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.2.16->jax-smi) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.2.16->jax-smi) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->jax-smi) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install snakeviz"
      ],
      "metadata": {
        "id": "5AItV9-jvGw9",
        "outputId": "c1120629-7e45-44e5-950c-3c38970c583a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snakeviz\n",
            "  Downloading snakeviz-2.2.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: tornado>=2.0 in /usr/local/lib/python3.10/dist-packages (from snakeviz) (6.3.3)\n",
            "Downloading snakeviz-2.2.0-py2.py3-none-any.whl (283 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/283.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m276.5/283.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snakeviz\n",
            "Successfully installed snakeviz-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tensorflow_datasets as tfds\n",
        "from jax_smi import initialise_tracking\n",
        "import pandas as pd\n",
        "import pstats\n",
        "import jax.profiler\n",
        "from datetime import datetime\n",
        "import tensorboard\n"
      ],
      "metadata": {
        "id": "4MfwSmuxm901"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test matrix u and v to test memory allocation using jax-smi\n",
        "key = jax.random.PRNGKey(0)\n",
        "size = 1000\n",
        "num_choices = 50\n",
        "mat_u = jax.random.normal(jax.random.PRNGKey(0), (1000, 1000))  # as initialized in init_latent_factors\n",
        "mat_v = jax.random.normal(jax.random.PRNGKey(1), (1000, 1000))\n",
        "rows = jax.random.choice(key, jnp.arange(size), (num_choices,), replace=False)\n",
        "columns = jax.random.choice(key, jnp.arange(size), (num_choices,), replace=False)"
      ],
      "metadata": {
        "id": "UX70swt_rZxR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit  # Comment out for single-step debugging\n",
        "def mse_loss_one_batch_test(mat_u, mat_v, rows, columns):\n",
        "    # Original code in question\n",
        "    jax_smi.initialise_tracking()\n",
        "    estimator = -(mat_u @ mat_v)[(rows, columns)]\n",
        "\n",
        "    return estimator\n",
        "\n",
        "mse_loss_one_batch_test(mat_u, mat_v, rows, columns)\n"
      ],
      "metadata": {
        "id": "LoTA9nYYo3Ir",
        "outputId": "87607483-cddd-44e5-ca7c-63714d2fa90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([  2.2403035,   5.137396 , -18.212055 ,  29.806274 ,  -5.4359016,\n",
              "        23.092825 ,   7.4908133, -33.361286 ,  10.951368 ,  -9.150698 ,\n",
              "       -14.942288 , -26.868412 , -39.69823  ,  39.272816 ,   1.6286244,\n",
              "        -5.6750965,  -7.0394726, -16.87818  ,  40.452354 ,  21.958443 ,\n",
              "         3.4872444, -39.629883 , -11.130831 , -70.76534  ,  -8.12375  ,\n",
              "         9.263904 ,   4.5230556,  33.51147  ,  15.698744 , -12.131374 ,\n",
              "        23.615482 , -38.220226 , -12.183689 , -46.414795 , -56.122215 ,\n",
              "         3.612556 ,  -8.421157 ,  -1.4403391, -22.431568 , -25.093353 ,\n",
              "        40.38521  ,  13.682131 ,  13.957107 ,  13.326932 ,  79.68028  ,\n",
              "       -26.777143 , -15.372421 ,  21.608826 ,  19.776518 ,   2.97534  ],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WJthik4x0zM",
        "outputId": "b74fdbbd-5578-4d79-9313-d37762bb744b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-39-082791cceb04>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-082791cceb04>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pprof --web memory.prof\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wdJ6mrJmaSk"
      },
      "source": [
        "## Exercise 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjXUQsMymaSl"
      },
      "source": [
        "# a)\n",
        "The rating of a professor should be divided into multiple categories ( makes a good lecture, is a good supervisor etc.), as tchey an differ a lot. This means, each time a user rates a lecture,seminar, thesis supervision, the rating should only affect the categories it fits best. ( having had a bad thesis supervision is not important for the quality of the lecture the professor will organize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRoDGymImaSl"
      },
      "source": [
        "# b)\n",
        "\n",
        "if the artworks are items, then there should be tags describing the essence of each artwork like {scary,nature,goofy}. These can be used to find out which genres the user prefers and are usable in content recommender systems.\n",
        "\n",
        "A better focus than the artwork may be the artists themselves, If a user likes an artwork he will prefer other art made in the same style, thus being from the same artist, compared to different artwork which may have the same topics, but differ in style. Thus the algorithm should rather try to match user and artist compared to user and picture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzbis5_lmaSl"
      },
      "source": [
        "# c)\n",
        "There is no user-item pairing, it is a user-user pairing. This changes the problem dramatically, as both users have to rate each other, thus a bidirectional pairing has to be done.\n",
        "\n",
        " Features could be superficial ( haircolor/weight etc.), personal information(age, religion, etc.) and tastes/dislikes (music genres, food etc.).   \n",
        "\n",
        "\n",
        "Another difficulty is in the aggregation of non-superficial data, as it is hard to put into categories, but may be even the most important data of all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GsZ8UZTmaSl"
      },
      "source": [
        "# Exercise 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT4BCF4ImaSl"
      },
      "source": [
        "## a)\n",
        "\n",
        "**G (global effects)** will probably return a newly published and well rated movie - that is quite popular - as a recommendation. The R system (regional effects) would be more aligned with the users preferences in terms of genre, director, actors etc. These are more likely to align with the taste of the specific user.\n",
        "\n",
        "The **L system (local effects)** would be the most personalized, as it would take into account the users previous ratings and preferences and compare them with other users. Depending on the size of the user cluster these can include more niche movies.\n",
        "\n",
        "**The System R** probably produces the most niche and exotic items, as it can infer personalized, latent factors in user preferences. System L cann offer some niche recommendations and System G mostly sticks to widely popular items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKdstYcQmaSm"
      },
      "source": [
        "## b)\n",
        "\n",
        "Depending on the size of the Dataset, System G would probably be the most vulnerable to this attack. Thats because it bases recommendations on global ratings and trends. So if a lot of user rate a movie with 5 stars, it will be recommended to a lot of other users, regardless of their preferences. In contrast, System L is the most robust against this manipulation. The fake accounts would mainly affect the reommendations of the fake accounts within the fake cluster, so their impact on the overall system is limited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Rp-BoHmaSm"
      },
      "source": [
        "## c)\n",
        "Both System G and System L fail for grey sheep, as they are based on global trends or user clusters. The grey sheep are users that do not fit into any cluster and have preferences that are not aligned with the global trends. System R is the only one that can recommend items to grey sheep, as it is based on the users preferences and not on global trends or user clusters and might align from time to time. Black sheep are very hard to capture and neither of the systems can reliable recommend items to them.    \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "MMD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}